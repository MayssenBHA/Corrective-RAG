{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MayssenBHA/Corrective-RAG/blob/main/corrective_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2808a1ff",
      "metadata": {
        "id": "2808a1ff"
      },
      "source": [
        "# Corrective RAG Implementation\n",
        "\n",
        "This notebook implements a corrective RAG (Retrieval-Augmented Generation) system that:\n",
        "1. Retrieves relevant documents from a local vector database\n",
        "2. Grades document relevance using an LLM\n",
        "3. If documents are not relevant, transforms the query and searches the web\n",
        "4. Generates final answers using all gathered context\n",
        "\n",
        "**Tech Stack:**\n",
        "- **LLM:** Mistral API (free tier available)\n",
        "- **Vector DB:** ChromaDB (local, no external services needed)\n",
        "- **Web Search:** Rapid API\n",
        "- **Orchestration:** LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0c48ce1",
      "metadata": {
        "id": "a0c48ce1"
      },
      "source": [
        "## 1. Install Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89d11f7a",
      "metadata": {
        "id": "89d11f7a"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community langchain-mistralai chromadb pypdf2 beautifulsoup4 langgraph pydantic typing-extensions nest-asyncio tenacity pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6f79e78",
      "metadata": {
        "id": "c6f79e78"
      },
      "source": [
        "## 2. Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8faf590a",
      "metadata": {
        "id": "8faf590a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import tempfile\n",
        "import pprint\n",
        "from typing import Dict, TypedDict, List\n",
        "import nest_asyncio\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "import requests  # Added for RapidAPI\n",
        "\n",
        "# LangChain imports\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader, WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "# Enable nested asyncio for Jupyter\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a83719bb",
      "metadata": {
        "id": "a83719bb"
      },
      "source": [
        "## 3. Configuration and API Keys\n",
        "\n",
        "Set up your API keys here. Get them from:\n",
        "- **Mistral API:** https://console.mistral.ai/\n",
        "- **Rapid API:** https://rapidapi.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b974cd39",
      "metadata": {
        "id": "b974cd39"
      },
      "outputs": [],
      "source": [
        "# API Keys - Replace with your actual keys\n",
        "MISTRAL_API_KEY = \"\"\n",
        "RAPIDAPI_KEY = \"\"\n",
        "\n",
        "# Document path to load - Update this with your uploaded document path in Colab\n",
        "DOC_PATH = \"/content/2411.15146v1.pdf\"\n",
        "\n",
        "# Validate API keys\n",
        "if MISTRAL_API_KEY == \"your_mistral_api_key_here\":\n",
        "    print(\"‚ö†Ô∏è Please set your Mistral API key\")\n",
        "else:\n",
        "    print(\"‚úÖ Mistral API key configured\")\n",
        "\n",
        "if RAPIDAPI_KEY == \"your_rapidapi_key_here\":\n",
        "    print(\"‚ö†Ô∏è Please set your RapidAPI key\")\n",
        "else:\n",
        "    print(\"‚úÖ RapidAPI key configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d7fc2d8",
      "metadata": {
        "id": "4d7fc2d8"
      },
      "source": [
        "## 4. Initialize Models and Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc668c6",
      "metadata": {
        "id": "ecc668c6"
      },
      "outputs": [],
      "source": [
        "# Initialize Mistral LLM\n",
        "llm = ChatMistralAI(\n",
        "    model=\"mistral-small-latest\",\n",
        "    mistral_api_key=MISTRAL_API_KEY,\n",
        "    temperature=0,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "# Initialize embeddings (using free HuggingFace embeddings)\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Models initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "725d3209",
      "metadata": {
        "id": "725d3209"
      },
      "source": [
        "## 5. Document Loading and Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "907ffac6",
      "metadata": {
        "id": "907ffac6"
      },
      "outputs": [],
      "source": [
        "def load_documents(file_path: str) -> List[Document]:\n",
        "    \"\"\"Load documents from local file path.\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"‚ùå File not found: {file_path}\")\n",
        "            return []\n",
        "\n",
        "        file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "        if file_extension == '.pdf':\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif file_extension in ['.txt', '.md']:\n",
        "            loader = TextLoader(file_path, encoding='utf-8')\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
        "\n",
        "        return loader.load()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading document: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Load and process documents\n",
        "print(\"üìÑ Loading documents...\")\n",
        "print(f\"üìÇ Document path: {DOC_PATH}\")\n",
        "\n",
        "# Check if file exists\n",
        "if not os.path.exists(DOC_PATH):\n",
        "    print(f\"‚ö†Ô∏è Document not found at {DOC_PATH}\")\n",
        "    print(\"Please upload your document to Colab and update the DOC_PATH variable\")\n",
        "    docs = []\n",
        "else:\n",
        "    docs = load_documents(DOC_PATH)\n",
        "\n",
        "if docs:\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=100\n",
        "    )\n",
        "    all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Create ChromaDB vectorstore\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=all_splits,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=\"./chroma_db\"  # Local storage\n",
        "    )\n",
        "\n",
        "    # Create retriever\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "    print(f\"‚úÖ Successfully processed {len(all_splits)} document chunks\")\n",
        "else:\n",
        "    print(\"‚ùå No documents loaded\")\n",
        "    retriever = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2e81bb4",
      "metadata": {
        "id": "b2e81bb4"
      },
      "source": [
        "## 6. Define Graph State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be36b99a",
      "metadata": {
        "id": "be36b99a"
      },
      "outputs": [],
      "source": [
        "class GraphState(TypedDict):\n",
        "    \"\"\"State of the corrective RAG graph.\"\"\"\n",
        "    keys: Dict[str, any]\n",
        "\n",
        "print(\"‚úÖ Graph state defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f168a52e",
      "metadata": {
        "id": "f168a52e"
      },
      "source": [
        "## 7. Define RAG Workflow Nodes\n",
        "\n",
        "### Node 1: Retrieve Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "089b7836",
      "metadata": {
        "id": "089b7836"
      },
      "outputs": [],
      "source": [
        "def retrieve(state):\n",
        "    \"\"\"Retrieve documents based on user question.\"\"\"\n",
        "    print(\"üîç STEP: Retrieving documents...\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "\n",
        "    if retriever is None:\n",
        "        print(\"‚ùå No retriever available\")\n",
        "        return {\"keys\": {\"documents\": [], \"question\": question}}\n",
        "\n",
        "    documents = retriever.get_relevant_documents(question)\n",
        "    print(f\"üìã Retrieved {len(documents)} documents\")\n",
        "\n",
        "    return {\"keys\": {\"documents\": documents, \"question\": question}}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfa9f834",
      "metadata": {
        "id": "cfa9f834"
      },
      "source": [
        "### Node 2: Grade Document Relevance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "221e3d24",
      "metadata": {
        "id": "221e3d24"
      },
      "outputs": [],
      "source": [
        "def grade_documents(state):\n",
        "    \"\"\"Grade whether retrieved documents are relevant to the question and decide if web search is needed.\"\"\"\n",
        "    print(\"‚öñÔ∏è STEP: Grading document relevance...\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "\n",
        "    # Grading prompt\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are grading the relevance of a retrieved document to a user question.\n",
        "Return ONLY a JSON object with a \"score\" field that is either \"yes\" or \"no\".\n",
        "Do not include any other text or explanation.\n",
        "\n",
        "Document: {context}\n",
        "Question: {question}\n",
        "\n",
        "Rules:\n",
        "- Check for related keywords or semantic meaning\n",
        "- Use lenient grading to only filter clear mismatches\n",
        "- Return exactly like this example: {{\"score\": \"yes\"}} or {{\"score\": \"no\"}}\"\"\",\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    filtered_docs = []\n",
        "    relevant_count = 0\n",
        "    relevant_threshold = 2 # Require at least 2 relevant documents to skip web search\n",
        "\n",
        "    for doc in documents:\n",
        "        try:\n",
        "            response = chain.invoke({\"question\": question, \"context\": doc.page_content})\n",
        "\n",
        "            # Extract JSON from response\n",
        "            import re\n",
        "            json_match = re.search(r'\\{.*\\}', response)\n",
        "            if json_match:\n",
        "                response = json_match.group()\n",
        "\n",
        "            score = json.loads(response)\n",
        "\n",
        "            if score.get(\"score\") == \"yes\":\n",
        "                print(\"‚úÖ Document relevant\")\n",
        "                filtered_docs.append(doc)\n",
        "                relevant_count += 1\n",
        "            else:\n",
        "                print(\"‚ùå Document not relevant\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error grading document: {str(e)}\")\n",
        "            # On error, keep the document and treat as relevant for safety\n",
        "            filtered_docs.append(doc)\n",
        "            relevant_count += 1 # Assume relevant on error to be safe\n",
        "            continue\n",
        "\n",
        "    # Decide if web search is needed based on the relevant document count\n",
        "    search_needed = \"Yes\" if relevant_count < relevant_threshold else \"No\"\n",
        "    print(f\"üìä Relevant documents: {relevant_count}/{len(documents)}\")\n",
        "    print(f\"ü§î Relevant threshold for skipping web search: {relevant_threshold}\")\n",
        "\n",
        "\n",
        "    return {\"keys\": {\"documents\": filtered_docs, \"question\": question, \"run_web_search\": search_needed}}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c6dc450",
      "metadata": {
        "id": "9c6dc450"
      },
      "source": [
        "### Node 3: Transform Query (for better web search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2324ee26",
      "metadata": {
        "id": "2324ee26"
      },
      "outputs": [],
      "source": [
        "def transform_query(state):\n",
        "    \"\"\"Transform the query to produce a better question for web search.\"\"\"\n",
        "    print(\"üîÑ STEP: Transforming query for web search...\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "\n",
        "    # Query transformation prompt\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"Generate a search-optimized version of this question by\n",
        "analyzing its core semantic meaning and intent.\n",
        "\\n ------- \\n\n",
        "{question}\n",
        "\\n ------- \\n\n",
        "Return only the improved question with no additional text:\"\"\",\n",
        "        input_variables=[\"question\"],\n",
        "    )\n",
        "\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    better_question = chain.invoke({\"question\": question})\n",
        "\n",
        "    print(f\"üìù Original: {question}\")\n",
        "    print(f\"üéØ Improved: {better_question}\")\n",
        "\n",
        "    return {\"keys\": {\"documents\": documents, \"question\": better_question}}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ade0bb59",
      "metadata": {
        "id": "ade0bb59"
      },
      "source": [
        "### Node 4: Web Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8d6b43",
      "metadata": {
        "id": "7b8d6b43"
      },
      "outputs": [],
      "source": [
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def execute_rapidapi_search(query, limit=3):\n",
        "    url = \"https://google-search74.p.rapidapi.com/\"\n",
        "    headers = {\n",
        "        \"x-rapidapi-host\": \"google-search74.p.rapidapi.com\",\n",
        "        \"x-rapidapi-key\": RAPIDAPI_KEY\n",
        "    }\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"limit\": limit,\n",
        "        \"related_keywords\": \"true\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers, params=params)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "def web_search(state):\n",
        "    \"\"\"Web search based on the transformed question using RapidAPI Google Search.\"\"\"\n",
        "    print(\"üåê STEP: Performing web search via RapidAPI...\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "\n",
        "    try:\n",
        "        # Validate RapidAPI key\n",
        "        if not RAPIDAPI_KEY or RAPIDAPI_KEY == \"your_rapidapi_key_here\":\n",
        "            print(\"‚ö†Ô∏è RapidAPI key not provided - skipping web search\")\n",
        "            return {\"keys\": {\"documents\": documents, \"question\": question}}\n",
        "\n",
        "        # Execute search with retry logic\n",
        "        search_results = execute_rapidapi_search(question, limit=3)\n",
        "\n",
        "        if not search_results or \"results\" not in search_results:\n",
        "            print(\"‚ö†Ô∏è No search results found\")\n",
        "            return {\"keys\": {\"documents\": documents, \"question\": question}}\n",
        "\n",
        "        # Process results\n",
        "        web_results = []\n",
        "        for result in search_results[\"results\"]:\n",
        "            content = (\n",
        "                f\"Title: {result.get('title', 'No title')}\\n\"\n",
        "                f\"Content: {result.get('description', 'No content')}\\n\"\n",
        "                f\"Link: {result.get('link', '')}\\n\"\n",
        "            )\n",
        "            web_results.append(content)\n",
        "\n",
        "        # Create document from results\n",
        "        web_document = Document(\n",
        "            page_content=\"\\n\\n\".join(web_results),\n",
        "            metadata={\n",
        "                \"source\": \"rapidapi_google_search\",\n",
        "                \"query\": question,\n",
        "                \"result_count\": len(web_results)\n",
        "            }\n",
        "        )\n",
        "        documents.append(web_document)\n",
        "\n",
        "        print(f\"‚úÖ Added {len(web_results)} web search results via RapidAPI\")\n",
        "\n",
        "    except Exception as error:\n",
        "        print(f\"‚ùå Web search error: {str(error)}\")\n",
        "\n",
        "    return {\"keys\": {\"documents\": documents, \"question\": question}}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63619261",
      "metadata": {
        "id": "63619261"
      },
      "source": [
        "### Node 5: Generate Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d890507",
      "metadata": {
        "id": "2d890507"
      },
      "outputs": [],
      "source": [
        "def generate(state):\n",
        "    \"\"\"Generate answer using Mistral model.\"\"\"\n",
        "    print(\"‚ú® STEP: Generating final answer...\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question, documents = state_dict[\"question\"], state_dict[\"documents\"]\n",
        "\n",
        "    try:\n",
        "        # Create prompt template\n",
        "        prompt = PromptTemplate(\n",
        "            template=\"\"\"Based on the following context, please answer the question.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer:\"\"\",\n",
        "            input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "\n",
        "        # Combine all document content\n",
        "        context = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
        "\n",
        "        # Create and run chain\n",
        "        rag_chain = (\n",
        "            {\"context\": lambda x: context, \"question\": lambda x: question}\n",
        "            | prompt\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "        generation = rag_chain.invoke({})\n",
        "        print(\"‚úÖ Answer generated successfully\")\n",
        "\n",
        "        return {\n",
        "            \"keys\": {\n",
        "                \"documents\": documents,\n",
        "                \"question\": question,\n",
        "                \"generation\": generation\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error in generate function: {str(e)}\"\n",
        "        print(f\"‚ùå {error_msg}\")\n",
        "        return {\n",
        "            \"keys\": {\n",
        "                \"documents\": documents,\n",
        "                \"question\": question,\n",
        "                \"generation\": \"Sorry, I encountered an error while generating the response.\"\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc1e633c",
      "metadata": {
        "id": "dc1e633c"
      },
      "source": [
        "### Node 6: Decision Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cbb5d3d",
      "metadata": {
        "id": "2cbb5d3d"
      },
      "outputs": [],
      "source": [
        "def decide_to_generate(state):\n",
        "    \"\"\"Decide whether to generate directly or search the web first.\"\"\"\n",
        "    print(\"ü§î STEP: Deciding next action...\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    search = state_dict[\"run_web_search\"]\n",
        "\n",
        "    if search == \"Yes\":\n",
        "        print(\"‚û°Ô∏è Decision: Transform query and run web search\")\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        print(\"‚û°Ô∏è Decision: Generate answer directly\")\n",
        "        return \"generate\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d18ce904",
      "metadata": {
        "id": "d18ce904"
      },
      "source": [
        "## 8. Build the Corrective RAG Workflow Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebfe5cda",
      "metadata": {
        "id": "ebfe5cda"
      },
      "outputs": [],
      "source": [
        "# Create workflow graph\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)\n",
        "workflow.add_node(\"grade_documents\", grade_documents)\n",
        "workflow.add_node(\"generate\", generate)\n",
        "workflow.add_node(\"transform_query\", transform_query)\n",
        "workflow.add_node(\"web_search\", web_search)\n",
        "\n",
        "# Build graph connections\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"web_search\")\n",
        "workflow.add_edge(\"web_search\", \"generate\")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# Compile the app\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"‚úÖ Corrective RAG workflow graph created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23ba7c68",
      "metadata": {
        "id": "23ba7c68"
      },
      "source": [
        "## 9. Helper Functions for Output Formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09e399c0",
      "metadata": {
        "id": "09e399c0"
      },
      "outputs": [],
      "source": [
        "def format_document(doc: Document) -> str:\n",
        "    \"\"\"Format document for display.\"\"\"\n",
        "    return f\"\"\"\n",
        "Source: {doc.metadata.get('source', 'Unknown')}\n",
        "Content: {doc.page_content[:200]}...\n",
        "\"\"\"\n",
        "\n",
        "def format_state(state: dict) -> str:\n",
        "    \"\"\"Format state for pretty printing.\"\"\"\n",
        "    formatted = {}\n",
        "\n",
        "    for key, value in state.items():\n",
        "        if key == \"documents\":\n",
        "            formatted[key] = [format_document(doc) for doc in value]\n",
        "        else:\n",
        "            formatted[key] = value\n",
        "\n",
        "    return formatted\n",
        "\n",
        "print(\"‚úÖ Helper functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13094de5",
      "metadata": {
        "id": "13094de5"
      },
      "source": [
        "## 10. Test the Corrective RAG System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6389e92d",
      "metadata": {
        "id": "6389e92d"
      },
      "outputs": [],
      "source": [
        "# Test question\n",
        "test_question = \"What are the experiment results and ablation studies in this research paper?\"\n",
        "\n",
        "print(f\"üéØ Question: {test_question}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Run the corrective RAG pipeline\n",
        "inputs = {\n",
        "    \"keys\": {\n",
        "        \"question\": test_question,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Execute workflow and show step-by-step progress\n",
        "final_result = None\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        print(f\"\\nüìã Step '{key}' completed:\")\n",
        "        print(\"-\" * 40)\n",
        "        # Show formatted state for this step\n",
        "        formatted_state = format_state(value[\"keys\"])\n",
        "        for state_key, state_value in formatted_state.items():\n",
        "            if state_key == \"generation\":\n",
        "                print(f\"\\nüéØ Final Answer Preview: {state_value[:100]}...\")\n",
        "            elif state_key == \"documents\":\n",
        "                print(f\"\\nüìÑ Documents: {len(state_value)} items\")\n",
        "            else:\n",
        "                print(f\"\\n{state_key}: {state_value}\")\n",
        "        final_result = value\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üèÜ FINAL ANSWER:\")\n",
        "print(\"=\"*80)\n",
        "if final_result and 'generation' in final_result['keys']:\n",
        "    print(final_result['keys']['generation'])\n",
        "else:\n",
        "    print(\"No final generation produced.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f08bebe1",
      "metadata": {
        "id": "f08bebe1"
      },
      "source": [
        "## 11. Interactive Question-Answer Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d114b47",
      "metadata": {
        "id": "0d114b47"
      },
      "outputs": [],
      "source": [
        "def ask_corrective_rag(question: str, verbose: bool = True) -> str:\n",
        "    \"\"\"Ask a question to the corrective RAG system.\"\"\"\n",
        "    if not question.strip():\n",
        "        return \"Please provide a valid question.\"\n",
        "\n",
        "    inputs = {\"keys\": {\"question\": question}}\n",
        "\n",
        "    final_result = None\n",
        "    for output in app.stream(inputs):\n",
        "        for key, value in output.items():\n",
        "            if verbose:\n",
        "                print(f\"\\nüìã Step '{key}' completed\")\n",
        "            final_result = value\n",
        "\n",
        "    return final_result['keys'].get('generation', 'No answer generated.')\n",
        "\n",
        "print(\"‚úÖ Interactive function ready! Use ask_corrective_rag('your question') to ask questions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0360ce97",
      "metadata": {
        "id": "0360ce97"
      },
      "source": [
        "## 12. Example Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c973b6d",
      "metadata": {
        "id": "6c973b6d"
      },
      "outputs": [],
      "source": [
        "# Example questions to test\n",
        "example_questions = [\n",
        "    \"What is the main topic of the paper?\",\n",
        "    \"What is the capital of France?\",\n",
        "    \"Can you summarize the abstract of this paper?\",\n",
        "    \"What are the key limitations discussed?\",\n",
        "    \"Who is the author of 'Pride and Prejudice'?\",\n",
        "    \"What is the purpose of a vector database in RAG?\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing with example questions...\\n\")\n",
        "\n",
        "for i, question in enumerate(example_questions, 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Question {i}: {question}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    answer = ask_corrective_rag(question, verbose=False)\n",
        "    print(f\"\\nüéØ Answer: {answer}\")\n",
        "    print(\"\\n\" + \"-\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2acd0747",
      "metadata": {
        "id": "2acd0747"
      },
      "source": [
        "## 13. Custom Question Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c511192c",
      "metadata": {
        "id": "c511192c"
      },
      "outputs": [],
      "source": [
        "# Ask your own question\n",
        "your_question = input(\"Enter your question: \")\n",
        "\n",
        "if your_question:\n",
        "    print(f\"\\nüéØ Your Question: {your_question}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    answer = ask_corrective_rag(your_question, verbose=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üèÜ FINAL ANSWER:\")\n",
        "    print(\"=\"*80)\n",
        "    print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c10e7b9a",
      "metadata": {
        "id": "c10e7b9a"
      },
      "source": [
        "## Key Features of This Corrective RAG Implementation:\n",
        "\n",
        "### üîÑ **Corrective Mechanism:**\n",
        "1. **Initial Retrieval:** Gets relevant documents from local vector database\n",
        "2. **Relevance Grading:** LLM evaluates if retrieved docs are actually relevant\n",
        "3. **Correction:** If docs are poor, it improves the query and searches the web\n",
        "4. **Enhanced Generation:** Uses all available context to generate better answers\n",
        "\n",
        "### üõ†Ô∏è **Tech Stack:**\n",
        "- **LLM:** Mistral API (free tier available)\n",
        "- **Vector DB:** ChromaDB (local, no external services)\n",
        "- **Embeddings:** HuggingFace (free)\n",
        "- **Web Search:** Rapid API\n",
        "- **Orchestration:** LangGraph for workflow management\n",
        "\n",
        "### üéØ **Benefits:**\n",
        "- **Self-Correcting:** Automatically improves retrieval quality\n",
        "- **Hybrid Approach:** Combines local documents + web search\n",
        "- **Robust:** Handles cases where initial retrieval fails\n",
        "- **Cost-Effective:** Uses free/affordable APIs\n",
        "\n",
        "### üìù **Next Steps:**\n",
        "1. Set your API keys in the configuration cell\n",
        "2. Run all cells in order\n",
        "3. Test with the example questions\n",
        "4. Ask your own questions using the interactive function"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}